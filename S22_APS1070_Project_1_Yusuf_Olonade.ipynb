{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "S22_APS1070_Project_1_Yusuf_Olonade.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08owH-AVyZCX"
      },
      "source": [
        "# APS1070\n",
        "#### Project 1 --- Basic Principles and Models \n",
        "**Deadline: Jun 4, 11PM - 10 percent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRZ3wBoe398S"
      },
      "source": [
        "**Academic Integrity**\n",
        "\n",
        "This project is individual - it is to be completed on your own. If you have questions, please post your query in the APS1070 Piazza Q&A forums (the answer might be useful to others!).\n",
        "\n",
        "Do not share your code with others, or post your work online. Do not submit code that you have not written yourself. Students suspected of plagiarism on a project, midterm or exam will be referred to the department for formal discipline for breaches of the Student Code of Conduct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qonbueFGyemb"
      },
      "source": [
        "Name: Yusuf Olonade\n",
        "\n",
        "Student ID: 1006814743"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dykrw3cyy7PF"
      },
      "source": [
        "##**Marking Scheme:**\n",
        "\n",
        "This project is worth **10 percent** of your final grade.\n",
        "\n",
        "Draw a plot or table where necessary to summarize your findings. \n",
        "\n",
        "**Practice Vectorized coding**: If you need to write a loop in your solution, think about how you can implement the same functionality with vectorized operations. Try to avoid loops as much as possible (in some cases, loops are inevitable).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to submit **(HTML + IPYNB)**\n",
        "\n",
        "1. Download your notebook: `File -> Download .ipynb`\n",
        "\n",
        "2. Click on the Files icon on the far left menu of Colab\n",
        "\n",
        "3. Select & upload your `.ipynb` file you just downloaded, and then obtain its path (right click) (you might need to hit the Refresh button before your file shows up)\n",
        "\n",
        "\n",
        "4. execute the following in a Colab cell:\n",
        "```\n",
        "%%shell\n",
        "jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb\n",
        "```\n",
        "\n",
        "5. An HTML version of your notebook will appear in the files, so you can download it.\n",
        "\n",
        "6. Submit **both** <font color='red'>`HTML` and `IPYNB`</font>  files on Quercus for grading.\n",
        "\n",
        "\n",
        "\n",
        "Ref: https://stackoverflow.com/a/64487858 \n",
        "\n"
      ],
      "metadata": {
        "id": "dBW_phAt5wYk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYlo7mIXyZFe"
      },
      "source": [
        "# Project 1 [10 Marks] \n",
        "Let's apply the tools we have learned in Tutorial 1 to a new dataset.\n",
        "\n",
        "We're going to work with a breast cancer dataset. Download it using the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFo8KVcryZFe"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "dataset = load_breast_cancer()"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOGmwyQNyZFh"
      },
      "source": [
        "## Part 1: Getting started [2 Marks]\n",
        "First off, take a look at the `data`, `target` and `feature_names` entries in the `dataset` dictionary. They contain the information we'll be working with here. Then, create a Pandas DataFrame called `df` containing the data and the targets with the feature names as column headings. If you need help, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for more details on how to achieve this. **[0.4]**\n",
        "* How many features do we have in this dataset? 30\n",
        "* How many observations have a 'mean area' of greater than 700? 171\n",
        "* How many participants tested `Malignant`? 212\n",
        "* How many participants tested `Benign`? 357"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "7GFE_Qw2598h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "477R6xyUbm0Z"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = dataset.feature_names\n",
        "feature_data = dataset.data\n",
        "target_names = dataset.target_names\n",
        "target_data = dataset.target"
      ],
      "metadata": {
        "id": "AcW0yWaRufza"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "GWY7WcoZJL2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.DataFrame(data = feature_data, columns = feature_names)\n",
        "\n",
        "df_1[\"target\"] = target_data\n",
        "\n",
        "df_1"
      ],
      "metadata": {
        "id": "s9R92ffaoExC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"No of features:\",feature_data.shape[1])\n",
        "print(\"No of observations with mean area greater than 700:\", (df['mean area'] > 700). sum())\n",
        "print(\"No of participants tested Malignant:\", (df['target'] == 0). sum())\n",
        "print(\"No of participants tested Benign:\", (df['target'] == 1). sum())"
      ],
      "metadata": {
        "id": "2ad2E6ILwBzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyqBEQXGq-B0"
      },
      "source": [
        "### Splitting the data\n",
        "It is best practice to have a training set (from which there is a rotating validation subset) and a test set. Our aim here is to (eventually) obtain the best accuracy we can on the test set (we'll do all our tuning on the training/validation sets, however.) \n",
        "\n",
        "**Split the dataset** into a train and a test set **\"70:30\"**, use **``random_state=0``**. The test set is set aside (untouched) for final evaluation, once hyperparameter optimization is complete. **[0.5]**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "JgJjpyAKI8JK"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_jNtkfce9Eg"
      },
      "source": [
        "### Effect of Standardization (Visual)\n",
        "Use `seaborn.lmplot` ([help here](https://seaborn.pydata.org/generated/seaborn.lmplot.html)) to visualize a few features of the training set. Draw a plot where the x-axis is ``worst smoothness``, the y-axis is ``worst fractal dimension,`` and the color of each datapoint indicates its class.  **[0.5]**\n",
        "\n",
        "Standardizing the data is often critical in machine learning. Show a plot as above, but with two features with very different scales. Standardize the data and plot those features again. What's different? Based on your observation, what is the advantage of standardization? **[0.6]**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J684emSV8mJd"
      },
      "source": [
        "# Training set in Pandas DataFrame\n",
        "\n",
        "df_2 = pd.DataFrame(data = X_train, columns = feature_names)\n",
        "\n",
        "df_2[\"target\"] = y_train\n",
        "\n",
        "df_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 1: worst smoothness vs worst fractal dimension\n",
        "\n",
        "import seaborn as sns; sns.set_theme(color_codes=True)\n",
        "g_1 = sns.lmplot(x = \"worst smoothness\", y = \"worst fractal dimension\", hue = \"target\", data = df_2)"
      ],
      "metadata": {
        "id": "iDGuext9MDbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2: mean concave points vs mean area before standardization\n",
        "\n",
        "g_2 = sns.lmplot(x = \"mean concave points\", y = \"mean area\", hue = \"target\", data = df_2)"
      ],
      "metadata": {
        "id": "eU3qhxJhXQVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train) #Fitting the scaler on X_train\n",
        "\n",
        "print(\"Scaler Mean:\", scaler.mean_)\n",
        "print(\"Scaler Var:\",  scaler.var_ , \"\\n\")\n",
        "\n",
        "# Standardizing X_train & X_test\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "OTBazWP3ZjR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Casting the standardized training set in a Pandas DataFrame\n",
        "\n",
        "df_3 = pd.DataFrame(data = X_train_scaled, columns = feature_names)\n",
        "\n",
        "df_3[\"target\"] = y_train\n",
        "\n",
        "df_3"
      ],
      "metadata": {
        "id": "JWP69o3FdBfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 3: mean concave points vs mean area after standardization\n",
        "\n",
        "g_3 = sns.lmplot(x = \"mean concave points\", y = \"mean area\", hue = \"target\", data = df_3)"
      ],
      "metadata": {
        "id": "PSlzyT-ydifF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Visualizing Plot 2 above, before standardization, the scales of `mean area` is very large compared to `mean concave point`. After standardization in Plot 3, the scales are now same. The advantage of standardization is to prevent large feature values from dominating the small feature values which their effect in real life is equally important for our model prediction."
      ],
      "metadata": {
        "id": "r9-Df2TRhrwF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBmo0-W1yZFs"
      },
      "source": [
        "## Part 2: KNN Classifier without Standardization [2 Marks]\n",
        "Normally, standardizing data is a key step in preparing data for a KNN classifier. However, for educational purposes, let's first try to build a model without standardization. Let's create a KNN classifier to predict whether a patient has a malignant or benign tumor. \n",
        "\n",
        "Follow these steps: \n",
        "\n",
        "1.   Train a KNN Classifier using cross-validation on the dataset. Sweep `k` (number of neighbours) from 1 to 100, and show a plot of the mean cross-validation accuracy vs `k`. **[1]**\n",
        "2.   What is the best `k`? **k = 10**. What is the highest cross-validation accuracy? **0.9346518987341772** **[0.5]**\n",
        "3. Comment on  which ranges of `k` lead to underfitted or overfitted models (hint: compare training and validation curves!). **[0.5]**. **Ans:** `k` range between 75 to 100 have are underfitted because they have low training and validation score below 0.9 while `k` range between 1 to 4 are overfitted since they have high training score above 0.95 and low validation score below 0.92\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyEV-mVSAMaV"
      },
      "source": [
        "# Validating the kNN model with k = 1\n",
        "\n",
        "from sklearn import neighbors\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "knn_test = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
        "scores = cross_validate(knn_test, X_train, y_train, cv=5, return_train_score=True)\n",
        "\n",
        "print('Mean Train Accuracy:',scores['train_score'].mean()) \n",
        "print('Mean Validation Accuracy:', scores['test_score'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creates a new knn model\n",
        "knn_1 = neighbors.KNeighborsClassifier()\n",
        "\n",
        "# Creates a dictionary of all k-values we want to test\n",
        "param_grid = {'n_neighbors': np.arange(1, 101)}\n",
        "\n",
        "# Uses gridsearch to test all k-values we defined in dictionary\n",
        "clf_1 = GridSearchCV(knn_1, param_grid, return_train_score = True)\n",
        "\n",
        "# Fit model to our training data\n",
        "clf_1.fit(X_train, y_train)\n",
        "\n",
        "clf_1.cv_results_"
      ],
      "metadata": {
        "id": "8jt98EIe2clb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation results imported into a pd Dataframe\n",
        "\n",
        "df_4 = pd.DataFrame(clf_1.cv_results_)[['param_n_neighbors','mean_train_score', 'mean_test_score']]\n",
        "\n",
        "df_4"
      ],
      "metadata": {
        "id": "deJfnjDmAprs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 4: KNN Model Validation Curve\n",
        "\n",
        "df_4.plot.scatter(x ='param_n_neighbors', y = 'mean_test_score', c = 'Red', title = 'KNN Model Validation Curve')"
      ],
      "metadata": {
        "id": "0ngbWmT7Vqmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 5: KNN Model Training Curve\n",
        "\n",
        "df_4.plot.scatter(x ='param_n_neighbors', y='mean_train_score', c = 'Green', title = 'KNN Model Training Curve')"
      ],
      "metadata": {
        "id": "UIBbfWFJWzIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the best k parameter\n",
        "\n",
        "clf_1.best_params_"
      ],
      "metadata": {
        "id": "_CKkXubYlKhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the average mean validation score(accuracy) for the best k parameter\n",
        "\n",
        "Full_feature_val_score = clf_1.best_score_\n",
        "Full_feature_val_score"
      ],
      "metadata": {
        "id": "eUxU1ySmlKfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCyFm5S3srCO"
      },
      "source": [
        "## Part 3: Feature Selection [3 Marks]\n",
        "In this part, we aim to investigate the importance of each feature on the final classification accuracy. \n",
        "If we want to try every possible combination of features, we would have to test  $2^F$ different cases,  where F is the number of features, and in each case, we have to do a hyperparameter search (finding K, in KNN using cross-validation). That will take days!. \n",
        "\n",
        "To find more important features we will use a decision tree. based on a decision tree we can compute feature importance that is a metric for our feature selection (code is provided below).\n",
        "\n",
        "You can use [this link](https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
        ") to get familiar with extracting the feature impotance order of machine learning algorithms in Python.\n",
        "\n",
        "After we identified and removed the least important feature and evaluated a new KNN model on the new set of features, if the stop conditions (see step 7 below) are not met, we need to repeat the process and remove another feature.\n",
        "\n",
        "\n",
        "Design a function ( `Feature_selector`) that accepts your dataset (X_train , y_train) and a threshold as inputs and: **[1]**\n",
        "1. Fits a decision tree classifier on the training set.\n",
        "\n",
        "2. Extracts the feature importance order of the decision tree model.\n",
        "\n",
        "3. Removes the least important feature based on step 2. \n",
        "4. Then, a KNN model is trained on the remaining features. The number of neighbors (`k`) for each KNN model should be tuned using a 5-fold cross-validation.\n",
        "5. Store the best `mean cross-validation` score, the corresponding `k` (number of neighbours) value, and the removed feature in three lists.\n",
        "6. Repeat Steps 3-5 until you meet the stop condition (step 7). \n",
        " \n",
        "7. We will stop this process when (1) there is only one feature left, or (2) our cross-validation accuracy is dropped significantly compared to a model that uses all the features. In this function, we accept a threshold as an input argument. For example, if threshold=0.95 we do not continue removing features if our mean cross-validation accuracy after tuning `k` is bellow **0.95 $\\times$ Full Feature cross-validation accuracy**.\n",
        "\n",
        "8. Your function returns the list of removed features, the list of corresponding mean cross-validation accuracies, and the list of `k` values when a feature was removed (i.e., the lists that were appended to in Step 5).\n",
        "\n",
        "* Visualize your results by plotting the best mean cross-validation accuracy (based on the best value of `k`) on y axis vs. the number of features (x axis). This plot describes: what is the best cv score with 1 feature, 2 features, 3 features ... and all the features. **[0.5]**\n",
        "\n",
        "* Plot the best value of `k` (y-axis) vs. the number of features. This plot explains the trend of number of neighbours with respect to the number of features.  **[0.5]**\n",
        "\n",
        "* State what is the number of essential features for classification and justify your answer. **[1]**\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPcG6_UIdAaT"
      },
      "source": [
        "You can use the following piece of code to start training a decision tree classifier and obtain its feature importance order. \n",
        "```\n",
        "from sklearn import tree\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "dt.fit(X_train,y_train)\n",
        "importance = dt.feature_importances_\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates KNN model that is to be trained with selected features\n",
        "knn_2 = neighbors.KNeighborsClassifier()\n",
        "param_grid = {'n_neighbors': np.arange(1, 101)}\n",
        "clf_2 = GridSearchCV(knn_2, param_grid)"
      ],
      "metadata": {
        "id": "6mF1TkAf6T3T"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn.feature_selection import RFE        \n",
        "\n",
        "a = list(np.arange(1, 30))\n",
        "\n",
        "def Feature_selector (X_train, y_train, tr=0.95):\n",
        "  dt = tree.DecisionTreeClassifier()\n",
        "  dt.fit(X_train,y_train)\n",
        "  importance = dt.feature_importances_\n",
        "  best_score = 0.93                                                    # Full feature cross validation accuracy after tunning K\n",
        "  list_best_score = []\n",
        "  list_best_param = []\n",
        "  for i in a:\n",
        "    if best_score > (tr * best_score):\n",
        "      fs = RFE(dt, n_features_to_select=30-i, step=1)                   # Iterates the list a to select important feature\n",
        "      fs_trained = fs.fit(X_train, y_train)\n",
        "      X_train_fs = fs_trained.transform(X_train)                        # Stores X_train after transformation by RFE\n",
        "      clf_2 = GridSearchCV(knn_2, param_grid)                           # Defines KNN model to be trained\n",
        "      clf_2_trained = clf_2.fit(X_train_fs, y_train)\n",
        "      list_best_score.append(clf_2_trained.best_score_)\n",
        "      list_best_param.append(clf_2_trained.best_params_)\n",
        "      support = fs_trained.support_                                     # Returns a boolean array of selected features\n",
        "      support_inverted = np.invert(support)                             # Inverts the boolean array above\n",
        "      removed_features = feature_names[support_inverted]                # Returns an array of features removed\n",
        "  return list_best_score, list_best_param, list(removed_features)\n",
        "\n",
        "Feature_selector (X_train, y_train, tr=0.95)"
      ],
      "metadata": {
        "id": "cPfIRacJA-Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTXjb1sWmDZL"
      },
      "source": [
        "## Part 4: Standardization [1 Marks]\n",
        "\n",
        "Standardizing the data usually means scaling our data to have a mean of zero and a standard deviation of one. \n",
        "\n",
        "**Note:** When we standardize a dataset, do we care if the data points are in our training set or test set? Yes! The training set is available for us to train a model - we can use it however we want. The test set, however, represents a subset of data that is not available for us during training. For example, the test set can represent the data that someone who bought our model would use to see how the model performs (which they are not willing to share with us).\n",
        "Therefore, we cannot compute the mean or standard deviation of the whole dataset to standardize it - we can only calculate the mean and standard deviation of the training set. However, when we sell a model to someone, we can say what our scalers (mean and standard deviation of our training set) was. They can scale their data (test set) with our training set's mean and standard deviation. Of course, there is no guarantee that the test set would have a mean of zero and a standard deviation of one, but the model should still work well enough.\n",
        "\n",
        "**To summarize: We fit the StandardScaler only on the training set. We transform both training and test sets with that scaler.**\n",
        "\n",
        "1. Standardize the training  and test data ([Help](https://scikit-learn.org/stable/modules/preprocessing.html)) \n",
        "\n",
        "2. Call your ``Feature_selector`` function on the standardized training data with a threshold of 95\\%. \n",
        " * Plot the Cross validation accuracy when we have the standardized data (this part) and the original training data (last part) vs. the Number of features in a single plot (to compare them easily).\n",
        "\n",
        "3. Discuss how standardization (helped/hurt) your model and its performance? Discuss which cases lead to a higher cross validation accuracy (how many features? which features? What K?)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAwAGR6_2bu3"
      },
      "source": [
        "# 1. Standardizing the training and test set\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train) #Fitting the scaler on X_train\n",
        "\n",
        "print(\"Scaler Mean:\", scaler.mean_)\n",
        "print(\"Scaler Var:\",  scaler.var_ , \"\\n\")\n",
        "\n",
        "# Standardizing X_train & X_test\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature_selector function called on the standardized data above\n",
        "\n",
        "Feature_selector (X_train_scaled, y_train, tr=0.95)"
      ],
      "metadata": {
        "id": "g1CmBAw1ooJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Decision Tree Classifier [1.5 Mark]\n",
        "\n",
        "Train a decision tree classifier on the standardized dataset (read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and check the example there.) Tune the `max_depth` and `min_samples_split` parameters of the tree using cross-validation (CV).\n",
        " * Compare the decision tree's performance (mean CV score) with KNN, both using all the features. **Ans**: The KNN algorithm had a mean CV score of **0.97** while the decision tree had **0.92** hence the KNN algorithm performed better.\n"
      ],
      "metadata": {
        "id": "HdmuHF4kGH5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Creates a decision tree classifier object\n",
        "dt_2 = DecisionTreeClassifier()\n",
        "\n",
        "# Creates a dictionary of all max_depth and min_samples_split we want to tune\n",
        "param_grid_2 = {'max_depth': np.arange(1, 11), 'min_samples_split': np.arange(2, 21)}\n",
        "\n",
        "# Uses GridSearchCV to tune the parameters we defined in dictionary\n",
        "clf_2 = GridSearchCV(dt_2, param_grid_2, return_train_score = True)\n",
        "\n",
        "# Fit decision tree algorithm to our standardized training data\n",
        "clf_2.fit(X_train_scaled, y_train)\n",
        "\n",
        "clf_2.cv_results_"
      ],
      "metadata": {
        "id": "laYRhTK4Kytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the best parameter, observe that parameters were changing for each run\n",
        "\n",
        "clf_2.best_params_"
      ],
      "metadata": {
        "id": "xgPUb1dL9blm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the average mean validation score(accuracy) for the best k parameter\n",
        "\n",
        "clf_2.best_score_"
      ],
      "metadata": {
        "id": "_PvVkMLT_LMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Decision tree algorithm on the standardized training data after tunning max_depth to be 4 and min_samples_split to be 3\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "dt_4_3 = DecisionTreeClassifier(max_depth = 4, min_samples_split = 3)\n",
        "\n",
        "score_2 = cross_val_score(dt_4_3, X_train_scaled, y_train, cv=5)\n",
        "\n",
        "score_2.mean()"
      ],
      "metadata": {
        "id": "UyxvTtJpBmA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting KNN algorithm on the standardized training data after tunning K to be 10\n",
        "\n",
        "knn_10 = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "score_3 = cross_val_score(knn_10, X_train_scaled, y_train, cv=5)\n",
        "\n",
        "score_3.mean()"
      ],
      "metadata": {
        "id": "r0xDMmu4_LKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7QjKjHn2TZR"
      },
      "source": [
        "## Part 6: Test Data [0.5 Mark]\n",
        "\n",
        "Now that you've created several models, pick your best one (highest CV accuracy) and apply it to the test dataset you had initially set aside. Discuss your results. **Ans:** Test accuracy score was 95.91 %, which is very close to the training score of 96.74 %. From this, we see that our model is neither over-fitted nor under-fitted but performed satisfactorily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdJ7EijBYqB"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "knn_10.fit(X_train_scaled, y_train)\n",
        "accuracy = accuracy_score(y_test, knn_10.predict(X_test_scaled))\n",
        "print (\"Test set accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhCf82AOe3IS"
      },
      "source": [
        "References:\n",
        "\n",
        "https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/02/machine-learning-101-decision-tree-algorithm-for-classification/"
      ]
    }
  ]
}